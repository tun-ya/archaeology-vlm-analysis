# GPT-4o

## Evaluation command
```bash
    cd models/gpt_4o
    python ../../models/eval_script/bleu_score_evaluation.py results_processed_data.json
```
```
Average BLEU Score: 0.00012356713542529928
```

```bash
    cd models/gpt_4o
    python ../../models/eval_script/metric_score_evaluation.py results_processed_data.json
```
```
Average F1 score: 0.0410
Average Precision: 0.0304
Average Recall: 0.1214
```