# Evaluation on demo dataset


```bash
    cd demo/gemini_pro
    python ../../models/eval_script/bleu_score_evaluation.py results_demo.json
```

Average BLEU Score: 0.0


```bash
    cd demo/gemini_pro
    python ../../models/eval_script/metric_score_evaluation.py results_demo.json
```

Average F1 score: 0.0288
Average Precision: 0.0152
Average Recall: 0.4167
