# Evaluation on demo dataset


```bash
    cd demo/qwen2_vl_7b
    python ../../models/eval_script/bleu_score_evaluation.py results_demo.json
```

Average BLEU Score: 0.0


```bash
    cd demo/qwen2_vl_7b
    python ../../models/eval_script/metric_score_evaluation.py results_demo.json
```

Average F1 score: 0.0405
Average Precision: 0.0219
Average Recall: 0.4583
