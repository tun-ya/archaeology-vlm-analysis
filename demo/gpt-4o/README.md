# Evaluation on demo dataset


```bash
    cd demo/gpt-4o
    python ../../models/eval_script/bleu_score_evaluation.py results_demo.json
```

Average BLEU Score: 0.0


```bash
    cd demo/gpt-4o
    python ../../models/eval_script/metric_score_evaluation.py results_demo.json
```

Average F1 score: 0.0270
Average Precision: 0.0152
Average Recall: 0.1250

